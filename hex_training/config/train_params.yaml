# Model Parameters
model_name: model_idle
model: PPO
total_timesteps: 300000
env: Hexapod-v0-idle
learning_starts: 100
learning_rate: 0.0003
training_step : 5000
#### PPO Hyper parameters
PPO:
  gamma: 0.99
  nsteps: 50
  n_epochs: 1
  gae_lambda: 0.95
  clip_range: 0.2
  clip_range_vf: 0.2
  ent_coef: 0.001
  vf_coef: 0.5
  max_grad_norm: 0.5

#### DDPG Hyper parameters
DDPG:
  tau: 0.005
  gamma: 0.99
  batch_size: 100
  train_freq_number: 5
  train_freq_unit: step
  learning_starts: 100
  learning_rate: 0.001

SAC:
  alpha: 0.95
  tau: 1e-5
  batch_size: 100

# A2C Hyper parameters
a2c:
  gamma: 0.99
  n_steps: 10
  vf_coef: 0.5
  ent_coef: 0.001
  max_grad_norm: 0.5
  learning_rate: 0.0007
  max_grad_norm: 0.5
  gae_lambda: 0.95
  rms_prop_eps: 1e-5


#### Callback Parameters
### Eval Callback
eval_freq: 5000
eval_episodes: 20
deterministic: True

#### Environment Parameters
desired_pose:
    x: 0
    y: 0
    z: 0.8

desired_yaw: 0.0 # Desired yaw in radians for the hopper to stay
max_oriantation: 0.1     # in rads
running_step: 2.5   # in seconds
abs_joint_max_action: 2.3
  # in radians
abs_min_pos_dist: 0.1
abs_max_pos_dist: 0.9
abs_max_distance: 10

coxa_joint_max: 0.707
coxa_joint_min: -0.707
femur_joint_max: 0.707
femur_joint_min: -0.707
tibia_joint_max: 0.0
tibia_joint_min: -2.3

### Reward Parameters

## Reward 1: Distance from desired point 
## Reward 2: A negative reward for penalising the ant if the external contact force is too large. 
## Reward 2: A negative reward for penalising orientation 
done_reward: 40.0 # reward
alive_reward: 50.0 # reward

weight_r1: 100.0 
weight_r2: 8.0 
weight_r3: 100.0
